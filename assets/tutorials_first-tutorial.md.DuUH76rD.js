import{_ as o,c as e,o as r,ae as n}from"./chunks/framework.CBTkueSR.js";const u=JSON.parse('{"title":"YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南）","description":"","frontmatter":{},"headers":[],"relativePath":"tutorials/first-tutorial.md","filePath":"tutorials/first-tutorial.md"}'),a={name:"tutorials/first-tutorial.md"};function i(l,t,p,d,c,s){return r(),e("div",null,[...t[0]||(t[0]=[n(`<h1 id="yolov5-pt转engine核心-rtx环境依赖配置全攻略-附避坑指南" tabindex="-1">YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南） <a class="header-anchor" href="#yolov5-pt转engine核心-rtx环境依赖配置全攻略-附避坑指南" aria-label="Permalink to &quot;YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南）&quot;">​</a></h1><p>在实时目标检测场景里，YOLOv5训练生成的PT权重虽便捷，但推理速度远不能发挥RTX显卡的性能——而TensorRT优化后的Engine格式，能让推理效率提升30%-50%。网上充斥着PT转Engine的完整流程教程，但<strong>没人系统讲过依赖配置的门道</strong>。我在Ubuntu 24.04上反复踩坑十余次才打通流程，发现依赖版本匹配、安装顺序、源配置这三大块正是卡壳重灾区。这篇就聚焦最关键的依赖配置，帮你绕开90%的启动失败问题。</p><h2 id="一、先厘清概念-别搞混tensorrtx与tensorrt" tabindex="-1">一、先厘清概念：别搞混TensorRTX与TensorRT <a class="header-anchor" href="#一、先厘清概念-别搞混tensorrtx与tensorrt" aria-label="Permalink to &quot;一、先厘清概念：别搞混TensorRTX与TensorRT&quot;">​</a></h2><ul><li><p><strong>TensorRT</strong>：NVIDIA官方推出的深度学习推理优化SDK，核心作用是对模型进行量化、剪枝等优化，生成高效的推理引擎（Engine文件），是“加速能力”的核心提供者。</p></li><li><p><strong>TensorRTX</strong>：wang-xinyu 开发的开源项目（<a href="https://github.com/wang-xinyu/tensorrtx" target="_blank" rel="noreferrer">官方仓库</a>），为YOLOv5等热门模型提供了适配TensorRT的代码实现，相当于“桥梁”——让我们能快速把YOLOv5的PT权重通过TensorRT转成Engine。</p></li><li><p><strong>关键逻辑</strong>：我们要配的依赖，本质是让“桥梁（TensorRTX）”能正常调用“加速核心（TensorRT）”，而这一切都依赖NVIDIA显卡的底层支持（驱动、CUDA、cuDNN）。</p></li></ul><h2 id="二、核心-rtx环境依赖配置全流程-附实测有效版本" tabindex="-1">二、核心：RTX环境依赖配置全流程（附实测有效版本） <a class="header-anchor" href="#二、核心-rtx环境依赖配置全流程-附实测有效版本" aria-label="Permalink to &quot;二、核心：RTX环境依赖配置全流程（附实测有效版本）&quot;">​</a></h2><p>先放我最终打通的<strong>版本匹配表</strong>（重中之重！版本不匹配会报各种玄学错误，别乱换）：</p><table tabindex="0"><thead><tr><th>组件</th><th>实测有效版本</th><th>作用说明</th></tr></thead><tbody><tr><td>操作系统</td><td>Ubuntu 24.04 LTS</td><td>稳定且对新显卡支持友好，建议用官方中国镜像源安装</td></tr><tr><td>NVIDIA显卡驱动</td><td>580系列（如580.xx.xx，选择非test、非server版本）</td><td>显卡底层驱动，需支持CUDA 12.0，选择非test、非server的稳定版本</td></tr><tr><td>CUDA</td><td>12.0（通过apt安装，对应cuda-12-0包）</td><td>显卡计算框架，TensorRT依赖其运行</td></tr><tr><td>cuDNN</td><td>8.9.2（for CUDA 12.0）</td><td>CUDA的深度学习加速库，提升TensorRT的卷积计算效率</td></tr><tr><td>TensorRT</td><td>8.6.1.6（TensorRT-8.6.1.6.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz）</td><td>核心优化工具，负责生成Engine文件</td></tr><tr><td>Python环境</td><td>Miniconda 24.3.0 + Python 3.10</td><td>管理依赖包，避免系统环境污染</td></tr><tr><td>PyTorch</td><td>适配CUDA 12.0的版本（建议从PyTorch官网获取）</td><td>加载PT权重，配合TensorRTX执行转换</td></tr></tbody></table><h3 id="_2-1-第一步-配置系统源-避免后续安装卡壳" tabindex="-1">2.1 第一步：配置系统源（避免后续安装卡壳） <a class="header-anchor" href="#_2-1-第一步-配置系统源-避免后续安装卡壳" aria-label="Permalink to &quot;2.1 第一步：配置系统源（避免后续安装卡壳）&quot;">​</a></h3><p>Ubuntu默认源在国内速度慢，先换成清华源，同时配置conda和pip源，后续安装一路丝滑：</p><ol><li><p><strong>系统源替换</strong>：打开“软件和更新”，将“下载自”改为“位于中国的服务器”，刷新缓存后更新： <code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></p></li><li><p><strong>conda源配置</strong>：安装Miniconda后，执行以下命令添加清华源：</p><pre><code> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/

 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/

 conda config --set show_channel_urls yes
</code></pre></li><li><p><strong>pip源配置</strong>：创建pip配置文件：</p><pre><code> mkdir -p ~/.config/pip

 echo &quot;[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple&quot; &gt; ~/.config/pip/pip.conf
</code></pre></li></ol><h3 id="_2-2-第二步-安装nvidia显卡驱动-最容易忽略的前置" tabindex="-1">2.2 第二步：安装NVIDIA显卡驱动（最容易忽略的前置） <a class="header-anchor" href="#_2-2-第二步-安装nvidia显卡驱动-最容易忽略的前置" aria-label="Permalink to &quot;2.2 第二步：安装NVIDIA显卡驱动（最容易忽略的前置）&quot;">​</a></h3><p>驱动是基础，必须先装且版本适配CUDA！经过实测，Ubuntu自带的“附加驱动”能精准匹配系统内核，安装更便捷稳定，推荐优先使用此方式，重点选择“非test、非server”的最新稳定版本：</p><ol><li><p>卸载旧驱动（如果之前手动装过或有残留）：</p><pre><code> sudo apt purge nvidia-* -y

 # 执行后重启电脑

 sudo reboot
</code></pre></li><li><p>打开“附加驱动”：通过Ubuntu搜索栏输入“附加驱动”打开工具，等待系统自动扫描适配的显卡驱动</p></li><li><p>选择驱动版本：在扫描结果中，选择“非test（测试版）、非server（服务器版）”的最新稳定版本，例如我选择的“NVIDIA Corporation: NVIDIA GeForce RTX XXX”对应的580系列驱动</p></li><li><p>应用驱动：点击“应用更改”，系统会自动下载并安装所选驱动，过程中需输入密码授权，等待安装完成（约5-10分钟，视网络速度而定）</p></li><li><p>重启生效：安装完成后，点击“重启”按钮，或手动执行<code>sudo reboot</code>重启电脑</p></li><li><p>验证：重启后打开终端，执行<code>nvidia-smi</code>，若能显示显卡型号、驱动版本（如580.xx.xx）及支持的CUDA版本（如“CUDA Version: 12.0”），则驱动安装成功</p></li></ol><h3 id="_2-3-第三步-安装cuda-12-0-严格匹配版本" tabindex="-1">2.3 第三步：安装CUDA 12.0（严格匹配版本） <a class="header-anchor" href="#_2-3-第三步-安装cuda-12-0-严格匹配版本" aria-label="Permalink to &quot;2.3 第三步：安装CUDA 12.0（严格匹配版本）&quot;">​</a></h3><p>CUDA版本必须和TensorRT、cuDNN对应，这里选12.0：</p><ol><li><p>从<a href="https://developer.nvidia.com/cuda-12.0.1-download-archive" target="_blank" rel="noreferrer">CUDA官网</a>下载12.0版本（选择Linux→x86_64→Ubuntu→22.04→runfile）</p></li><li><p>执行安装：严格按照官网步骤逐行代码执行即可。。。</p></li><li><p>。。。</p></li></ol><p>or 最简单安装：sudo apt install cuda</p><h3 id="_2-4-第四步-安装cudnn-8-9-2-cuda的-加速器" tabindex="-1">2.4 第四步：安装cuDNN 8.9.2（CUDA的“加速器”） <a class="header-anchor" href="#_2-4-第四步-安装cudnn-8-9-2-cuda的-加速器" aria-label="Permalink to &quot;2.4 第四步：安装cuDNN 8.9.2（CUDA的“加速器”）&quot;">​</a></h3><p>cuDNN是为深度学习优化的库，必须选对应CUDA 12.0的版本：</p><ol><li><p>从<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noreferrer">cuDNN官网</a>下载“cuDNN Library for Linux x86_64”（版本8.9.7，for CUDA 12.x）</p></li><li><p>执行安装：严格按照官网步骤逐行代码执行即可。。。</p></li><li><p>。。。</p></li></ol><h3 id="_2-5-第五步-安装tensorrt-8-6-1-6-核心转换工具" tabindex="-1">2.5 第五步：安装TensorRT 8.6.1.6（核心转换工具） <a class="header-anchor" href="#_2-5-第五步-安装tensorrt-8-6-1-6-核心转换工具" aria-label="Permalink to &quot;2.5 第五步：安装TensorRT 8.6.1.6（核心转换工具）&quot;">​</a></h3><p>TensorRT是生成Engine的关键，这里用tar包安装（比deb包更灵活）：</p><ol><li><p>从<a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download" target="_blank" rel="noreferrer">TensorRT官网</a>下载对应版本（TensorRT-8.x）</p></li><li><p>解压到指定目录：<code>tar -xzvf TensorRT-8.6.1.6.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz -C ~/tools/</code></p></li><li><p>配置环境变量：打开<code>~/.bashrc</code>，添加：</p><pre><code> export TENSORRT_DIR=~/tools/TensorRT-8.6.1.6

 export PATH=$TENSORRT_DIR/bin:$PATH

 export LD_LIBRARY_PATH=$TENSORRT_DIR/lib:$LD_LIBRARY_PATH
</code></pre></li><li><p>生效环境变量：<code>source ~/.bashrc</code></p></li><li><p>安装Python绑定：进入TensorRT的python目录，根据当前Python版本选择对应whl文件安装（以Python 3.10为例）：<code>cd ~/tools/TensorRT-8.6.1.6/python &amp;&amp; pip install tensorrt-8.6.1.6-cp310-none-linux_x86_64.whl</code></p></li><li><p>验证（可选）：Python中执行<code>import tensorrt as trt; print(trt.__version__)</code>，能输出8.6.1.6</p></li></ol><h3 id="_2-6-第六步-配置python环境-加载pt权重用" tabindex="-1">2.6 第六步：配置Python环境（加载PT权重用） <a class="header-anchor" href="#_2-6-第六步-配置python环境-加载pt权重用" aria-label="Permalink to &quot;2.6 第六步：配置Python环境（加载PT权重用）&quot;">​</a></h3><p>用Miniconda创建独立环境，避免依赖冲突：</p><ol><li><p>创建环境：<code>conda create -n yolov5-trt python=3.10 -y</code></p></li><li><p>激活环境：<code>conda activate yolov5-trt</code></p></li><li><p><strong>安装PyTorch</strong>：建议前往<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noreferrer">PyTorch官网</a>，根据实际环境（CUDA 12.0、Python 3.10、pip）选择对应配置，复制官网提供的pip安装命令执行（官网命令会适配最新兼容版本，避免手动指定版本可能出现的适配问题）。</p></li><li><p><strong>安装YOLOv5依赖</strong>：推荐通过YOLOv5官方的requirements.txt文件安装，确保依赖版本与YOLOv5适配。先克隆YOLOv5项目获取配置文件（若已克隆可跳过）：<code>git clone https://github.com/ultralytics/yolov5.git</code>，进入项目目录后执行：<code>pip install -r requirements.txt</code>（若需指定版本，可编辑requirements.txt后再安装）。</p></li></ol><h2 id="三、避坑指南-我踩过的5个致命错误-附解决方案" tabindex="-1">三、避坑指南：我踩过的5个致命错误（附解决方案） <a class="header-anchor" href="#三、避坑指南-我踩过的5个致命错误-附解决方案" aria-label="Permalink to &quot;三、避坑指南：我踩过的5个致命错误（附解决方案）&quot;">​</a></h2><p>这些问题网上搜不到明确答案，都是我实测踩坑总结的，遇到直接照解：</p><p><strong>坑1：通过apt安装CUDA时未清理旧驱动，导致冲突报错</strong></p><p>解决方案：apt安装CUDA时默认不会强制覆盖旧驱动，需先彻底清理残留：执行<code>sudo apt purge nvidia-* cuda-* -y</code>，重启后重新通过“附加驱动”装显卡驱动，再按步骤安装CUDA即可。</p><p><strong>坑2：Import tensorrt时报“libnvinfer.so.8: cannot open shared object file”</strong></p><p>解决方案：不是没装TensorRT，而是环境变量没生效！执行<code>source ~/.bashrc</code>重新加载，若还是报错，检查TENSORRT_DIR路径是否正确（确保和实际解压路径一致）。</p><p><strong>坑3：安装cuDNN后，执行TensorRT示例报错“CUDNN_STATUS_VERSION_MISMATCH”</strong></p><p>解决方案：cuDNN版本和CUDA不匹配！比如用了CUDA 12.0却装了for CUDA 11.8的cuDNN，重新下载对应版本的cuDNN（参考2.4步骤）。</p><p><strong>坑4：nvidia-smi显示CUDA Version 12.0，但nvcc -V显示11.7</strong></p><p>原因说明：</p><ul><li><code>nvidia-smi</code>显示的是<strong>显卡驱动支持的最高CUDA版本</strong>（此处12.0代表驱动可兼容≤12.0的CUDA）；</li><li><code>nvcc -V</code>显示的是<strong>当前系统实际生效的CUDA版本</strong>（此处11.7说明环境变量指向了旧版本）。</li></ul><p>两者不一致的核心问题是：系统中同时安装了多个CUDA版本（如11.7和12.0），但环境变量仍指向旧版本11.7，导致编译时调用的是旧版本。</p><p>解决方案：</p><ol><li>打开环境变量配置文件：<code>vim ~/.bashrc</code>（若不熟悉vim，可改用<code>gedit ~/.bashrc</code>图形化编辑）；</li><li>找到与CUDA相关的路径设置（如<code>export PATH=/usr/local/cuda-11.7/bin:$PATH</code>和<code>export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64:$LD_LIBRARY_PATH</code>）</li></ol><ul><li><p>删除旧版本（11.7）的配置，仅保留目标版本（12.0）的路径</p></li><li><p>例如： <code>export PATH=/usr/local/cuda-12.0/bin:$PATH </code> <code>export LD_LIBRARY_PATH=/usr/local/cuda-12.0/lib64:$LD_LIBRARY_PATH</code></p></li></ul><ol start="3"><li>保存文件后，执行<code>source ~/.bashrc</code>使配置生效，建议重启终端验证：</li></ol><ul><li>再次运行<code>nvcc -V</code>，确认显示版本为12.0；</li><li><code>nvidia-smi</code>显示的驱动支持版本无需修改，保持与实际安装的CUDA版本兼容即可（≤12.0）。</li></ul><p><strong>坑5：Pip安装torch时速度极慢，甚至超时</strong></p><p>原因说明： 安装PyTorch（尤其是GPU版本）时速度慢的核心原因通常有两点：</p><ol><li>直接使用官方默认源（pypi.org）时，服务器位于境外，国内网络访问可能存在延迟或带宽限制；</li><li>若配置了第三方镜像源（如清华源），可能因源地址错误、配置文件路径不正确（如非<code>~/.config/pip/pip.conf</code>或<code>~/.pip/pip.conf</code>），或镜像源同步延迟导致无法生效。</li></ol><p>需注意：PyTorch官网提供的安装命令（含<code>https://download.pytorch.org/whl/</code>源）是针对GPU版本的官方推荐方式，该源本身无访问限制，但国内网络环境可能仍存在连接不稳定的问题。</p><p>解决方案：</p><ol><li><strong>优先使用官网命令+国内镜像加速</strong>：</li></ol><ul><li>例外：从PyTorch官网（<a href="https://pytorch.org/%EF%BC%89%E8%8E%B7%E5%8F%96%E5%AF%B9%E5%BA%94CUDA" target="_blank" rel="noreferrer">https://pytorch.org/）获取对应CUDA</a> 12.0版本的安装命令（如<code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code>），</li></ul><ol start="2"><li><strong>检查第三方镜像源配置</strong>：</li></ol><ul><li>若需长期使用镜像源，确认<code>pip.conf</code>路径正确（Linux通常为<code>~/.config/pip/pip.conf</code>或<code>~/.pip/pip.conf</code>），文件内容示例： <code>[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple [install] </code> <code>trusted-host = pypi.tuna.tsinghua.edu.cn</code> 保存后重新执行安装命令。</li></ul><ol start="3"><li><strong>临时解决网络问题</strong>：</li></ol><ul><li>若上述方法仍超时，可尝试切换网络环境（如使用有线连接），或通过代理工具提升境外连接稳定性。</li></ul><h2 id="四、总结与后续-依赖配好-转换就成功了80" tabindex="-1">四、总结与后续：依赖配好，转换就成功了80% <a class="header-anchor" href="#四、总结与后续-依赖配好-转换就成功了80" aria-label="Permalink to &quot;四、总结与后续：依赖配好，转换就成功了80%&quot;">​</a></h2><p>很多人卡在PT转Engine的第一步，不是代码有问题，而是依赖配置没到位——毕竟NVIDIA的这套生态对版本匹配要求极高，差一个小版本都可能报错。我上面给的版本表和步骤都是实测跑通的，只要严格照做，基本能绕开所有基础坑。</p><p>配好依赖后，后续的PT转Engine核心步骤可参考官方教程（附录已提供权威指引）。如果执行中遇到其他问题，欢迎评论区留言，我会把你的问题补充到避坑指南里，帮助更多人少走弯路～</p><h2 id="附录-pt转engine核心流程指引" tabindex="-1">附录：PT转Engine核心流程指引 <a class="header-anchor" href="#附录-pt转engine核心流程指引" aria-label="Permalink to &quot;附录：PT转Engine核心流程指引&quot;">​</a></h2><p>完成前文依赖配置后，PT转Engine的核心流程可直接参考TensorRTX官方提供的YOLOv5专属教程，该教程会根据YOLOv5版本动态更新适配步骤，比通用流程更精准权威：</p><p>官方教程链接：<a href="https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5" target="_blank" rel="noreferrer">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p><p>官方教程核心优势：① 明确标注各YOLOv5版本（v5/v6/v7等）对应的分支切换方法；② 提供权重转换、编译推理的完整命令及常见问题解答；③ 实时更新适配最新系统环境的操作细节。</p><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><ul><li><p>[1] TensorRTX官方仓库. <a href="https://github.com/wang-xinyu/tensorrtx" target="_blank" rel="noreferrer">https://github.com/wang-xinyu/tensorrtx</a></p></li><li><p>[2] NVIDIA CUDA 12.0官方文档. <a href="https://docs.nvidia.com/cuda/12.0/index.html" target="_blank" rel="noreferrer">https://docs.nvidia.com/cuda/12.0/index.html</a></p></li><li><p>[3] YOLOv5配置与训练笔记. <a href="https://www.cnblogs.com/tokepson/p/18817469" target="_blank" rel="noreferrer">https://www.cnblogs.com/tokepson/p/18817469</a></p></li></ul><blockquote><p>（注：文档部分内容可能由 AI 生成，有错误请联系作者修改）</p></blockquote>`,64)])])}const T=o(a,[["render",i]]);export{u as __pageData,T as default};
